{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/posts/from-map-reduce-to-apache-spark/","result":{"data":{"markdownRemark":{"id":"85cda3c2-8bb5-50e0-b9cd-6f55995128a8","html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/e04f82a3fee3798b54e5559aac0666fa/4b190/apache-spark.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 60%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB143KGIr/xAAZEAACAwEAAAAAAAAAAAAAAAABEAIDETP/2gAIAQEAAQUC5xNoer//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAYEAADAQEAAAAAAAAAAAAAAAAAASERIP/aAAgBAQAGPwJtvaQvH//EABoQAAMBAAMAAAAAAAAAAAAAAAABESExQXH/2gAIAQEAAT8hU7yKR43w2KNg8xcFUrZ//9oADAMBAAIAAwAAABAvz//EABcRAAMBAAAAAAAAAAAAAAAAAAABESH/2gAIAQMBAT8QujZ//8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERIf/aAAgBAgEBPxBLCH//xAAbEAEAAwEAAwAAAAAAAAAAAAABABExIVGBsf/aAAgBAQABPxBsgvRpbnqMAgkL6ru/IWcB9AYwjQBaVC+WUHiOxn//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/e04f82a3fee3798b54e5559aac0666fa/8ac56/apache-spark.webp 240w,\n/static/e04f82a3fee3798b54e5559aac0666fa/d3be9/apache-spark.webp 480w,\n/static/e04f82a3fee3798b54e5559aac0666fa/d00b9/apache-spark.webp 800w\"\n              sizes=\"(max-width: 800px) 100vw, 800px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/e04f82a3fee3798b54e5559aac0666fa/09b79/apache-spark.jpg 240w,\n/static/e04f82a3fee3798b54e5559aac0666fa/7cc5e/apache-spark.jpg 480w,\n/static/e04f82a3fee3798b54e5559aac0666fa/4b190/apache-spark.jpg 800w\"\n            sizes=\"(max-width: 800px) 100vw, 800px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/e04f82a3fee3798b54e5559aac0666fa/4b190/apache-spark.jpg\"\n            alt=\"spark\"\n            title=\"\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>The <strong>MapReduce</strong> has been the standard for processing large data sets for years. <strong>Apache Hadoop</strong> is an open-source framework that implements the MapReduce model for distributed computing.</p>\n<p>The model of MapReduce splits the whole data calculation into two phases: the map phase and the reduce phase. The map phase processes the input data and produces a set of key-value pairs, while the reduce phase processes the output of the map phase and produces the final result. For both phases, the data is calculated in parallel, then the results are combined and written to disk. This model is simple and easy to understand, but its limitations are obvious. The most significant limitation is that it is slow for iterative algorithms, such as machine learning algorithms because it writes intermediate results to disk after each iteration.</p>\n<p>This is where <strong>Apache Spark</strong> comes in. The major change in Apache Spark is that it uses in-memory processing, which makes it faster than MapReduce. It introduces new concepts like <em>Spark Job</em>, <em>Spark Context</em>, and <em>Spark Session</em>, which provide a more user-friendly API for working with large data sets. Then <em>Spark Stage</em> and <em>Spark Task</em> are introduced to optimize the execution of the job. Spark also introduces new data structures like <em>Resilient Distributed Datasets (RDDs)</em> and <em>DataFrames</em>, which are more efficient than MapReduce’s model.</p>\n<h2 id=\"spark-job\" style=\"position:relative;\"><a href=\"#spark-job\" aria-label=\"spark job permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Job</h2>\n<p>A <strong>Spark Job</strong> is a set of transformations and actions on data that are executed in parallel on a cluster of machines. It is the basic unit of work in Apache Spark. A Spark Job is composed of one or more stages, which are divided based on the shuffle boundaries. Each stage is composed of one or more tasks, which are executed on individual partitions of the data.</p>\n<h2 id=\"spark-context--spark-session\" style=\"position:relative;\"><a href=\"#spark-context--spark-session\" aria-label=\"spark context  spark session permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spark Context &#x26; Spark Session</h2>\n<p>A <strong>Spark Context</strong> is the entry point to the Spark API. It is used to create RDDs, broadcast variables, and accumulators. The Spark Context is responsible for managing the resources of the Spark application, such as memory, CPU, and disk. It also coordinates the execution of the Spark Job on the cluster. A <strong>Spark Session</strong> is a unified entry point to the Spark API. It is introduced in Spark 2.0 to replace the Spark Context and SQL Context. The Spark Session provides a more user-friendly API for working with structured data, such as DataFrames and Datasets. It also provides a more optimized execution engine called <strong>Catalyst</strong>.</p>\n<h2 id=\"resilient-distributed-datasets-rdds\" style=\"position:relative;\"><a href=\"#resilient-distributed-datasets-rdds\" aria-label=\"resilient distributed datasets rdds permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Resilient Distributed Datasets (RDDs)</h2>\n<p><strong>RDDs</strong> are the core data structure in Apache Spark. They are immutable distributed collections of objects. Each dataset in an RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. RDDs support two types of operations: transformations and actions. Transformations create a new dataset from an existing one, while actions return a value to the driver program after running a computation on the dataset. RDDs are fault-tolerant because they track the lineage of the dataset, so they can be reconstructed if a partition is lost.</p>\n<h2 id=\"dataframes\" style=\"position:relative;\"><a href=\"#dataframes\" aria-label=\"dataframes permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DataFrames</h2>\n<p><strong>DataFrames</strong> are a distributed collection of data organized into named columns. They are similar to tables in a relational database or data frames in Python. DataFrames can be created from a variety of data sources, such as structured data files, tables in Hive, external databases, or existing RDDs. They provide a more user-friendly API for working with structured data. DataFrames are more efficient than RDDs because they use the Catalyst optimizer to optimize the execution of the job.</p>\n<h2 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h2>\n<p>Apache Spark has provided a more efficient and user-friendly data processing engine to replace the traditional MapReduce. It’s more efficient to write a Spark Job than a MapReduce Job.</p>","fields":{"slug":"/posts/2017-07-05---From-MapReduce-to-Apache-Spark//posts/from-map-reduce-to-apache-spark","tagSlugs":["/tag/programming/","/tag/apache-spark/"]},"frontmatter":{"date":"2017-07-05T17:00:00.000Z","description":"Apache Spark is a well designed and efficient data processing engine to replace the traditional MapReduce.","tags":["Programming","Apache Spark"],"title":"From MapReduce to Apache Spark","socialImage":{"publicURL":"/static/e04f82a3fee3798b54e5559aac0666fa/apache-spark.jpg"}}}},"pageContext":{"slug":"/posts/2017-07-05---From-MapReduce-to-Apache-Spark//posts/from-map-reduce-to-apache-spark"}},"staticQueryHashes":["251939775","288581551","401334301"],"slicesMap":{}}