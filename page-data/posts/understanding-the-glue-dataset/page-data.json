{"componentChunkName":"component---src-templates-post-template-post-template-tsx","path":"/posts/understanding-the-glue-dataset/","result":{"data":{"markdownRemark":{"id":"d13bdf29-e8ee-5251-ba0b-e5dd4cc105ac","html":"<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/07eaf8eddbd531224c1ceb16ca03aeb6/4b190/glue.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 15%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAADABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAMF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdmYlgv/xAAYEAACAwAAAAAAAAAAAAAAAAAAAQIxMv/aAAgBAQABBQKWVZ//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAQ/9oACAEBAAY/An//xAAYEAADAQEAAAAAAAAAAAAAAAABETEAEP/aAAgBAQABPyEGT0bef//aAAwDAQACAAMAAAAQ/wDP/8QAFREBAQAAAAAAAAAAAAAAAAAAARD/2gAIAQMBAT8QZ//EABYRAAMAAAAAAAAAAAAAAAAAAAEQMf/aAAgBAgEBPxARf//EABoQAQABBQAAAAAAAAAAAAAAAAERABAhQVH/2gAIAQEAAT8QAImCSjuw6bf/2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/07eaf8eddbd531224c1ceb16ca03aeb6/8ac56/glue.webp 240w,\n/static/07eaf8eddbd531224c1ceb16ca03aeb6/d3be9/glue.webp 480w,\n/static/07eaf8eddbd531224c1ceb16ca03aeb6/d00b9/glue.webp 800w\"\n              sizes=\"(max-width: 800px) 100vw, 800px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/07eaf8eddbd531224c1ceb16ca03aeb6/09b79/glue.jpg 240w,\n/static/07eaf8eddbd531224c1ceb16ca03aeb6/7cc5e/glue.jpg 480w,\n/static/07eaf8eddbd531224c1ceb16ca03aeb6/4b190/glue.jpg 800w\"\n            sizes=\"(max-width: 800px) 100vw, 800px\"\n            type=\"image/jpeg\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/07eaf8eddbd531224c1ceb16ca03aeb6/4b190/glue.jpg\"\n            alt=\"glue\"\n            title=\"\"\n            loading=\"lazy\"\n            decoding=\"async\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p><strong>GLUE</strong> stands for General Language Understanding Evaluation. It is a benchmark dataset for evaluating the performance of models across a diverse set of existing natural language understanding tasks. The dataset is designed to encourage the development of models that can perform a wide range of natural language understanding tasks. The dataset consists of nine tasks, including:</p>\n<ol>\n<li><strong>CoLA</strong> (Corpus of Linguistic Acceptability): A binary classification task where the goal is to predict whether a sentence is grammatically correct or not.</li>\n<li><strong>SST-2</strong> (Stanford Sentiment Treebank): A binary classification task where the goal is to predict whether a sentence has a positive or negative sentiment.</li>\n<li><strong>MRPC</strong> (Microsoft Research Paraphrase Corpus): A binary classification task where the goal is to predict whether two sentences are paraphrases of each other.</li>\n<li><strong>QQP</strong> (Quora Question Pairs): A binary classification task where the goal is to predict whether two questions are semantically equivalent.</li>\n<li><strong>STS-B</strong> (Semantic Textual Similarity Benchmark): A regression task where the goal is to predict the similarity between two sentences on a scale from 0 to 5.</li>\n<li><strong>MNLI</strong> (Multi-Genre Natural Language Inference): A three-way classification task where the goal is to predict whether a premise entails, contradicts, or is neutral with respect to a hypothesis.</li>\n<li><strong>QNLI</strong> (Question-answering Natural Language Inference): A binary classification task where the goal is to predict whether a question can be answered by a given context.</li>\n<li><strong>RTE</strong> (Recognizing Textual Entailment): A binary classification task where the goal is to predict whether a premise entails a hypothesis.</li>\n<li><strong>WNLI</strong> (Winograd Schema Challenge): A binary classification task where the goal is to predict whether a sentence exhibits coreference resolution.</li>\n</ol>\n<p>The official website for the GLUE benchmark dataset is <a href=\"https://gluebenchmark.com\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">here</a>. The website provides detailed information about the dataset, including the tasks, the evaluation metrics, and the leaderboard. The website also provides links to the datasets and the evaluation scripts.</p>","fields":{"slug":"/posts/2019-04-24---Understanding-the-GLUE-Dataset//posts/understanding-the-glue-dataset","tagSlugs":["/tag/machine-learning/","/tag/natural-language-processing/"]},"frontmatter":{"date":"2019-04-24T16:00:00.000Z","description":"a summary of the General Language Understanding Evaluation (GLUE) benchmark dataset.","tags":["Machine Learning","Natural Language Processing"],"title":"Understanding the GLUE Dataset","socialImage":{"publicURL":"/static/07eaf8eddbd531224c1ceb16ca03aeb6/glue.jpg"}}}},"pageContext":{"slug":"/posts/2019-04-24---Understanding-the-GLUE-Dataset//posts/understanding-the-glue-dataset"}},"staticQueryHashes":["251939775","288581551","401334301"],"slicesMap":{}}